{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IW2eLcHZElq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee2930c3-9063-4125-876c-1d839bb9f07c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m810.5/810.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.4/124.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Hugging Face Transformers is an open-source framework for deep learning created by Hugging Face.\n",
        "# It provides APIs and tools to download state-of-the-art pre-trained models and further tune them to maximize performance.\n",
        "# These models support common tasks in different modalities, such as natural language processing, computer vision, audio, and multi-modal applications.\n",
        "# Using pretrained models can reduce your compute costs, carbon footprint,\n",
        "# and save you the time and resources required to train a model from scratch.\n",
        "\n",
        "# https://huggingface.co/docs/transformers/index\n",
        "# https://huggingface.co/docs/hub/index\n",
        "\n",
        "# Accelerate library to help users easily train a 🤗 Transformers model on any type of distributed setup,\n",
        "# whether it is multiple GPU's on one machine or multiple GPU's across several machines.\n",
        "\n",
        "!pip install -q transformers langchain==0.1.13 huggingface-hub==0.21.4 accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrZaaNnpCWld"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y6C2vOtNBJK"
      },
      "outputs": [],
      "source": [
        "# we need to login to Hugging Face to have access to their inference API.\n",
        "# This step requires a free Hugging Face token.\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o787jKFhKTFq"
      },
      "outputs": [],
      "source": [
        "# This class provides functionality related to Hugging Face Transformers pipelines .\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "# This line imports the AutoTokenizer class from the transformers library.\n",
        "# The AutoTokenizer class is used to load tokenizers for various pre-trained language models available in the Hugging Face model hub.\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# This line imports the entire transformers library, which is a popular library developed by\n",
        "# Hugging Face for working with various transformer-based models in natural language processing (NLP),\n",
        "# including both models and tokenizers.\n",
        "import transformers\n",
        "\n",
        "# This line imports the torch library, which is the primary library used for deep learning and tensor computations in PyTorch.\n",
        "import torch\n",
        "\n",
        "# Model name that we want to use\n",
        "# https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
        "\n",
        "model = \"meta-llama/Meta-Llama-3-8B\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "# Set up text generation pipeline\n",
        "pipeline = transformers.pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer= tokenizer,\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                device_map=\"auto\",\n",
        "                max_new_tokens = 512,\n",
        "                do_sample=True,\n",
        "                top_k=10,\n",
        "                num_return_sequences=1,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "# logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "template = \"\"\"You're assisting someone in finding the perfect perfume tailored to their preferences.\n",
        "Write a brief paragraph describing what they're looking for using straightforward language, avoiding emojis.\n",
        "Consider their preferred fragrance, the season they plan to wear it, their age group,\n",
        "the time of day they'll use it, and their gender.\n",
        "\n",
        "For instance:\n",
        "\n",
        "\"I am a male looking for a perfume which has floral scent and can be used in both day and night.The perfume should be suited for summer and also has spicy note to it.\"\n",
        "\n",
        "Feel free to adjust the specifics of the scenario, but ensure it focuses on narratively describing the customer's needs without requiring structured input.\n",
        "              Following is the details of the customer needs:\n",
        "              Needs :\n",
        "              [\n",
        "                fragrance : {fragrance};\n",
        "                season : {season};\n",
        "                age : {age};\n",
        "                time : {time};\n",
        "                gender : {gender}.\n",
        "              ]\n",
        "              Based on above details generate the text\"\"\"\n",
        "\n",
        "# # pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "# result = pipeline(f\"<s>[INST] {prompt} [/INST]\")\n",
        "# print(result[0]['generated_text'])\n",
        "\n",
        "fragrance1 = \"ozonic, balsamic, spicy , woody , fresh\"\n",
        "season1 = \"winter\"\n",
        "age1 = \"adult\"\n",
        "time1 = \"day, night\"\n",
        "gender1 = \"female\"\n",
        "\n",
        "prompt1 = template.format(fragrance=fragrance1, season=season1, age=age1, time=time1, gender=gender1)\n",
        "# Run text generation pipeline with the updated inputs\n",
        "result1 = pipeline(f\"<s>[INST] {prompt1} [/INST]\")\n",
        "print(result1[0]['generated_text'])"
      ],
      "metadata": {
        "id": "VbD_iu6V6ru9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_text = result1[0]['generated_text'].split(\"[/INST]\")[1].strip()\n",
        "print(extracted_text)"
      ],
      "metadata": {
        "id": "Ef1PzZfD88Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the index of the beginning of the paragraph\n",
        "start_index = extracted_text.find('\"')\n",
        "\n",
        "# Find the index of the end of the paragraph\n",
        "end_index = extracted_text.rfind('\\\"') + 1\n",
        "\n",
        "# Extract the paragraph\n",
        "extracted_paragraph = extracted_text[start_index:end_index]\n",
        "\n",
        "# Print the extracted paragraph\n",
        "print(extracted_paragraph.strip('\\\"'))"
      ],
      "metadata": {
        "id": "mdvVtyEG9YzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getLLMResponse(prompt):\n",
        "  result = pipeline(f\"<s>[INST] {prompt} [/INST]\")\n",
        "  return result[0]['generated_text']"
      ],
      "metadata": {
        "id": "pmisblws-EFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getPrompt(fragrance, season, age, time, gender):\n",
        "  prompt = template.format(fragrance=fragrance, season=season, age=age, time=time, gender=gender)\n",
        "  print(prompt)\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "6UW7idTR-Qy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getTheRealOutput(fragrance, season, age, time, gender):\n",
        "  prompt = getPrompt(fragrance, season, age, time, gender)\n",
        "  response = getLLMResponse(prompt)\n",
        "  text = response.split(\"[/INST]\")[1].strip()\n",
        "  start_index = text.find('\"')\n",
        "\n",
        "# Find the index of the end of the paragraph\n",
        "  end_index = text.rfind('\\\"') + 1\n",
        "\n",
        "# Extract the paragraph\n",
        "  extractedParagraph = text[start_index:end_index]\n",
        "\n",
        "# Print the extracted paragraph\n",
        "  print(extractedParagraph)\n",
        "  return extractedParagraph.strip('\\\"')\n"
      ],
      "metadata": {
        "id": "XyoMb13h-etv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "L9Y4uB7hf9vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/Normalized_Data (1).csv\")"
      ],
      "metadata": {
        "id": "nYwYLxaqgEZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "cITs7EdIhYJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "  text = text.lower()\n",
        "  return text"
      ],
      "metadata": {
        "id": "3J1DDODsgNj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_input(fragrance,season,age,time,gender):\n",
        "  input_dict = {\n",
        "    \"fragrance\": fragrance,\n",
        "    \"season\": season,\n",
        "    \"age\": age,\n",
        "    \"time\": time,\n",
        "    \"gender\": gender\n",
        "    }\n",
        "  return input_dict"
      ],
      "metadata": {
        "id": "RTVus8vjhC_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['fragrance'] = df['accords'].fillna('') +','+ df['top_notes'].fillna('') +','+ df['middle_notes'].fillna('') +','+ df['base_notes'].fillna('')\n",
        "df['fragrance'] = df['fragrance'].apply(preprocess)\n",
        "df['season'] = df['season'].apply(preprocess)\n",
        "df['age'] = df['age'].apply(preprocess)\n",
        "df['time'] = df['time'].apply(preprocess)\n",
        "df['gender'] = df['gender'].apply(preprocess)"
      ],
      "metadata": {
        "id": "cEAXsvMhgfRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "fFxtE6KBhdeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['generated'] = df.apply(lambda x: getTheRealOutput(x['fragrance'],x['season'],x['age'],x['time'],x['gender']),axis=1)"
      ],
      "metadata": {
        "id": "lsC6N32tg8CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qMFbi29NzW6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = df[['fragrance','season','age','gender','time','generated']]"
      ],
      "metadata": {
        "id": "pcQPrNCkzia3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "AMDZLv-N0d-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the file path where you want to save the CSV file\n",
        "csv_file_path = 'GeneratedData.csv'\n",
        "\n",
        "# Save the entire DataFrame to a CSV file\n",
        "dataset.to_csv(csv_file_path, index=False)\n",
        "\n",
        "print(f\"DataFrame data has been saved to '{csv_file_path}'.\")"
      ],
      "metadata": {
        "id": "SY24vzSK0RuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the file path where you want to save the Excel file\n",
        "xlsx_file_path = 'GeneratedData.xlsx'\n",
        "\n",
        "# Save the entire DataFrame to an Excel file\n",
        "dataset.to_excel(xlsx_file_path, index=False)\n",
        "\n",
        "print(f\"DataFrame data has been saved to '{xlsx_file_path}'.\")\n"
      ],
      "metadata": {
        "id": "-ms1XLVC1LQ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}